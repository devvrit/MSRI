\section{Correlation Clustering solves Robust version upto constant approximation}

Let $CC(I)$ be the optimal clustering on the instance $I$, $RCC(I,m)$ be the optimal clustering after removing any $m$ vertices, and $\cost_{RCC}(I,m)$ be the associated cost with $RCC(I,m)$. 

% Consider a data set $N$, which is perfectly clusterable. Let this perfect clustering be denoted $\mathbf{C}_1$, composed of $m$ clusters $R_1, R_2, \dots, R_m$. Let us add $X$ new points to the original data points in order to disturb the clustering. $X$ are referred to as outliers. Let the optimal clustering on $N \cup X$ be denoted $\mathbf{C}_2$. Then, we show the following.

\begin{theorem}\label{theorem:17}
Consider an instance $I$ such that $\opt_{RCC}(I,k)=0$. Then $\exists \; O(k)$ vertices in any optimal solution to $\opt_{CC}(I)$ s.t. deleting them makes cluster $0$ cost.
\end{theorem}
Let $\mathcal{C}_1$ be the $RCC(I,k)$, and $\mathcal{C}_2$ be the $CC(I)$. Also let $\mathcal{C}_1$ contain $m$ clusters $R_1, R_2, \dots, R_m$. Let the set of $k$ vertices being removed be $X$. For clustering $\mathcal{C}_2$, the following three clusters are possible:
\begin{enumerate}
    \item[$c_1$:] contains $r_{11}(\neq \phi)\subseteq R_1$ and $x_1(\neq \phi) \subseteq X$.
    \item[$c_2$:] contains $r_{12(\neq \phi)}\subseteq R_1$for $2\le i \le m$, $r_i \subseteq R_i$ (not all equal to $\phi$) and $x_2(\neq \phi) \subseteq X$.
    \item[$c_3$:] $r_{13}\subseteq R_1$ in its own cluster.
\end{enumerate}
\begin{lemma}\label{lemma-d1}
% Consider cluster $c\in \mathcal{C}_2$, containing $r_{1}(\neq \phi) \subseteq R_1$, for $2\le i \le m$, $r_i \subseteq R_i$ (not all equal to $\phi$) and $x(\neq \phi)\subseteq X$.
For cluster $c_2$, $\sum_{i}r_i \in O(|x|)$.
\end{lemma}
\begin{proof}
Using the optimality of $\mathcal{C}_2$, the cost of $\mathcal{C}_2$ must increase when $c_2$ is separated into two components: one composed of $r_2 \cup r_3 \cup \dots \cup r_m \cup x $ and another with $r_{11}$ in its own cluster. By the optimality of $\mathcal{C}_2$, this clustering must have a higher cost:
    \begin{align}
        |r_{11}| \left( \sum_{j \ge 2} |r_j| \right) &\leq |x_2| |r_{11}| \nonumber\\
        \implies |r_2| + |r_3| + \cdots + |r_m| &\leq |x_2| \label{eq:00001}
    \end{align}
    Similarly,
    \begin{align}
        |r_{11}| + |r_3| + \cdots + |r_m| &\leq |x_2|, \nonumber\\
        \implies |r_{11}| &\leq |x_2| \label{eq:00002}
    \end{align}

    It follows from \eqref{eq:00001} and \eqref{eq:00002} that $|x_2| \ge \frac12 \sum_{i=1}^{m} |r_i|$.\\
\end{proof}

\begin{lemma}\label{lemma-d2}
% Let $c_1\in \mathcal{C}_2$ contain $r_{11} (\neq \phi)\subseteq R_1$ and $x_1 (\neq \phi)\subseteq X$. And $c_2 \in \mathcal{C}_2$ contain $r_{12}(\neq \phi)\subseteq R_1$ and $x_2(\neq \phi) \subseteq X$.
Consider the clusters $c_1$ and another cluster $c_{1}'$, similar to $c_1$, consisting $r_{11}'\subseteq (\neq \phi)R_1$, and $x_{1}' (\neq \phi) \subseteq X$.
Then $r_{11}+r_{11}'\in O(|x_1|+|x_1'|)$.
\end{lemma}
\begin{proof}
If $|r_{11}|\leq|r_{11}'|$, then merging $r_{11}$ with cluster $c_{2}$ must increase the cost.
    \begin{align*}
        &|r_{11}| |r_{11}'| \le (|x_1| + |x_{1}'|) |r_{11}|,\\
        \implies &|r_{11}'| \le |x_1| + |x_{1}'|,\\
        \implies &\frac12 (|r_{11}|+|r_{11}'|) \le |x_1| + |x_{1}'|.
    \end{align*}
\end{proof}
\begin{lemma}\label{lemma-d3}
Consider clusters $c_1$ and $c_3$, then $r_{11}\in O(|x_1|)$.
\end{lemma}
\begin{proof}
\begin{enumerate}
    \item If $|r_{11}| \le |r_{13}|$, then merging $r_{11}$ with $c_2$ must increase the cost. Therefore,
    \begin{align*}
        |r_{11}| &|r_{13}| \le |x| |r_{11}|,\\
        \implies &|r_{13}| \le |x|,\\
        \implies &|r_{11}| \le |x|.
    \end{align*}
    \item Else if $|r_{11}| \ge |r_{13}|$, then merging $|r_{13}|$ with $c_1$ gives,
    \begin{align*}
        |r_{11}| &|r_{13}| \le |x| |r_{13}|,\\
        \implies &|r_{11}| \le |x|,\\
    \end{align*}
\end{enumerate}
\end{proof}

\begin{proof}$\textit{to Theorem \ref{theorem:17}}$: 
We present the algorithm for removing $O(k)$ points in clustering $RCC(I,k)$.

\begin{algorithm}[H]\label{algo-d1}
\caption{}
\label{alg:dynamicf}
\begin{algorithmic}[1]
\State Remove all clusters of type $c_2$
\State Remove pairwise clusters of type $c_1$
\If{cluster of type $c_1$ and $c_3$ remains}
\State Remove cluster $c_1$
\ElsIf{Cluster of type only $c_1$ remains}
\State Remove $x_1$ points $\in c_1$
\EndIf
\end{algorithmic}
%\end{center}
\end{algorithm}


\end{proof}

% \begin{theorem} \label{theorem:17}
% A vertex is \textbf{bad} with respect to a clustering, if it has at least one mis-classified edge associated with it. Then, on any perfect clustering, it is possible to remove $\mathcal{O} (X)$ points from $\mathbf{C}_2$ and reduce its cost to $0$. Or in other words, the number of bad vertices with respect to $\mathbf{C}_2$ is $\mathcal{O} (X)$.
% \end{theorem}

% \begin{proof}
% For cluster in $C_2$, the following three clusters are possible:
% \begin{enumerate}
%     \item[$\mathbf{c}_1$:] contains $r_{11}\subseteq R_1$ and some $x_1 \subseteq X$.
%     \item[$\mathbf{c}_2$:] contains $r_{12}\subseteq R_1$ and for every $i \ge 2$, some $r_i \subseteq R_i$ (not all empty set) and some $x_2 \subseteq X$ (not empty set).
%     \item[$\mathbf{c}_3$:] $r_{13}\subseteq R_1$ in its own cluster.
% \end{enumerate}

% \begin{enumerate}
%     \item Consider cluster $c_2$. Using the optimality of $\mathbf{C}_2$, the cost of $\mathbf{C}_2$ must increase when $c_2$ is separated into two components: one composed of $r_2 \cup r_3 \cup \dots \cup r_m \cup x $ and another with $r_{12}$ in its own cluster. By the optimality of $\mathbf{C}_2$, this clustering must have a higher cost:
%     \begin{align}
%         |r_{12}| \left( \sum_{j \ge 2} |r_j| \right) &\leq |x| |r_{12}| \nonumber\\
%         \implies |r_2| + |r_3| + \cdots + |r_m| &\leq |x| \label{eq:00001}
%     \end{align}
%     Similarly,
%     \begin{align}
%         |r_{12}| + |r_3| + \cdots + |r_m| &\leq |x|, \nonumber\\
%         \implies |r_{12}| &\leq |x| \label{eq:00002}
%     \end{align}

%     %By symmetric arguments, we can claim that $\forall i = 1,2,\dots,m$, $|x| \ge |r_i|$.
%     It follows from \eqref{eq:00001} and \eqref{eq:00002} that $|x| \ge \frac12 \sum_{i=1}^{m} |r_i|$.\\
%     \item Consider cluster $c_1$ and a similar cluster, say $c_{1}'$ consisting $r_{11}'\subseteq R_1$, and $x_{1}' \neq \phi \subseteq X$. If $|r_{11}|\leq|r_{11}'|$, then merging $r_{11}$ with cluster $c_{1}'$ must increase the cost.
%     \begin{align*}
%         &|r_{11}| |r_{11}'| \le (|x_1| + |x_{1}'|) |r_{11}|,\\
%         \implies &|r_{11}'| \le |x_1| + |x_{1}'|,\\
%         \implies &\frac12 (|r_{11}|+|r_{11}'|) \le |x_1| + |x_{1}'|.
%     \end{align*}
        
%     \item Consider clusters $\mathbf{c}_1$ and $\mathbf{c}_3$. Then,
%     \begin{enumerate}
%         \item If $|r_{11}| \le |r_{13}|$, then merging $r_{11}$ with $\mathbf{c}_3$ must increase the cost. Therefore,
%         \begin{align*}
%             |r_{11}| &|r_{13}| \le |x_1| |r_{11}|,\\
%             \implies &|r_{13}| \le |x_1|.
%         \end{align*}
%         \item Else if $|r_{11}| \ge |r_{13}|$, then merging $|r_{13}|$ with $\mathbf{c}_1$ gives,
%         \begin{align*}
%             |r_{11}| &|r_{13}| \le |x_1| |r_{13}|,\\
%             \implies &|r_{11}| \le |x_1|,\\
%             \implies &|r_{13}| \le |x_1|.
%         \end{align*}
%     \end{enumerate}
% \end{enumerate}

% Now consider the following algorithm:
% \begin{enumerate}
%     \item Remove all the clusters of type $c_2$. We know this leads to removing only $O(|x|)$ vertices where $x$ is total number of outliers belonging to these clusters.
%     \item Select pair-wise clusters of type $c_1$ and remove them. We still remove $O(|x|)$ vertices where $x$ is total number of outliers belonging to these clusters.
%     \item Finally, either only cluster $c_3$ remains or one cluster of type $c_1$ and $c_3$ each. In the former case, remove the $x$ points from the cluster $c_1$. In the latter, remove the entire $c_1$. In both the case we remove only $O(|x|)$ vertices where $x$ is the number of outliers removed in this last step.
% \end{enumerate}
% \end{proof}

% So we see from the above two theorems that solving the correlation clustering optimally gives us a way of solving the robust problem too.

\section{Robust correlation clustering is interesting to study in an approximate setting}

Consider a dataset $D$ having $n$ points. $D$ is segregated into a $\sqrt{n} \times \sqrt{n}$ grid such that all points lying on a row are similar to each other, while points lying in different rows are dissimilar. The dataset, therefore can be clustered perfectly with each row forming an independent cluster. Let $\sqrt{n}$ outliers be added to the dataset with the intention of disturbing the clustering. Each outlier has a $+$ with all points in $N$ and $-$ with other outliers. For this new dataset, the optimal clustering will still be row clusters with one newly added point each. Hence, each cluster will now contain $\sqrt{n}+1$ points, and the cost of this optimal clustering is $O(n\sqrt{n})$. Now consider the other clustering: with each column as a cluster having one newly added point each. The cost of clustering is still $O(n\sqrt{n})$, but the vertex cover of bad points (as defined in previous sections) become $O(n)$. Hence, it is possible for an approximate optimal clustering to have a very large vertex cover compared to the original number of outliers that exist. Therefore, studying the robust correlation clustering problem is important.
